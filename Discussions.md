# Discussions about this task
## Bottlenecks
### Maximum length in number of tokens
The first bottlenecks I had met, long text will cause error
* I just let max_length fixed in this task
* A more effective approach should be improved from feature engineering, like filter URL, it need more referrnces
  * [BERT: How to Handle Long Documents](https://www.saltdatalabs.com/blog/bert-how-to-handle-long-documents)
  * [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/abs/1905.05583)
### Computing resource
Computing resources limit the size of each training data
* I limited batch size and used smaple data, than increased runs of training
* 
## Ideas for improvement performance
### 
###
## What I learned in this task
### Transformer
### Self-attention
## Design a framework for production
## References
* [BERT: How to Handle Long Documents](https://www.saltdatalabs.com/blog/bert-how-to-handle-long-documents)
* [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/abs/1905.05583)
